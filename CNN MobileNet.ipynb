{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "from glob import glob\n",
    "import re\n",
    "import ast\n",
    "import cv2\n",
    "import csv\n",
    "import time\n",
    "import ast\n",
    "import urllib\n",
    "from PIL import Image, ImageDraw\n",
    "from tqdm import tqdm\n",
    "from dask import bag, threaded\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as pltc\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from dask import bag, threaded\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from tensorflow.keras.metrics import categorical_accuracy, top_k_categorical_accuracy, categorical_crossentropy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications.nasnet import NASNetMobile\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "BASE_SIZE = 256\n",
    "DP_DIR = '../input/shuffle-csvs/'\n",
    "INPUT_DIR = '../input/quickdraw-doodle-recognition/'\n",
    "NCSVS = 100\n",
    "NCATS = 340\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "95cca8690a82b00d26f4e63a635e746e1f55389c"
   },
   "outputs": [],
   "source": [
    "startTime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#clean spaces in name\n",
    "classes_path = os.listdir(INPUT_DIR + 'train_simplified/')\n",
    "classes_path = sorted(classes_path, key=lambda s: s.lower())\n",
    "class_dict = {x[:-4].replace(\" \", \"_\"):i for i, x in enumerate(classes_path)}\n",
    "labels = {x[:-4].replace(\" \", \"_\") for i, x in enumerate(classes_path)}\n",
    "\n",
    "n_labels = len(labels)\n",
    "print(\"Number of labels: {}\".format(n_labels))\n",
    "\n",
    "fileList = glob(INPUT_DIR + \"train_simplified/*.csv\")     \n",
    "\n",
    "n_files = n_labels #number of csv files same as labels due to stupid structure.\n",
    "\n",
    "#time is sacred HARDCODED FOR THE COMP\n",
    "n_records = 49707919\n",
    "size = 80\n",
    "\n",
    "#for f in fileList: saving time\n",
    "#    n_records += sum(1 for line in open(f))\n",
    "print(\"Number of records: {}\".format(n_records))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4a5825305429b7f4095ba371467a477ae0a49472"
   },
   "outputs": [],
   "source": [
    "def top_3_accuracy(y_true, y_pred):\n",
    "    return top_k_categorical_accuracy(y_true, y_pred, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6c5311002def8107d5c73f866b399446a6a05ed4"
   },
   "outputs": [],
   "source": [
    "# to image from stroke\n",
    "def drawing_to_np(drawing, shape=(size, size)):\n",
    "    drawing = eval(drawing)\n",
    "    fig, ax = plt.subplots()\n",
    "    for x,y in drawing:\n",
    "        ax.plot(x, y, marker='.')\n",
    "        ax.axis('off')\n",
    "    fig.canvas.draw()\n",
    "    # Convert images to numpy arrat\n",
    "    np_drawing = np.array(fig.canvas.renderer._renderer)\n",
    "    plt.close(fig)\n",
    "    img = cv2.resize(np_drawing, shape)\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    img_expanded = img_gray[:, :, np.newaxis]\n",
    "    return img_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9102e216d298ee8d3f9c1ff7991ada57119e2323"
   },
   "outputs": [],
   "source": [
    "def draw_cv2_reshape_normalized(raw_strokes, size=size, lw=6):\n",
    "    img = np.zeros((BASE_SIZE, BASE_SIZE), np.uint8)\n",
    "    for stroke in raw_strokes:\n",
    "        for i in range(len(stroke[0]) - 1):\n",
    "            _ = cv2.line(img, (stroke[0][i], stroke[1][i]), (stroke[0][i + 1], stroke[1][i + 1]), 255, lw)\n",
    "\n",
    "    img = cv2.resize(img, (size, size))\n",
    "    img = img / 255.\n",
    "    img = img[:, :, np.newaxis]\n",
    "    return img\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "97522d9de29d9ff35c5086f55f2c8eef64d84f90"
   },
   "outputs": [],
   "source": [
    "def draw_cv2(raw_strokes, size=256, lw=6):\n",
    "    img = np.zeros((BASE_SIZE, BASE_SIZE), np.uint8)\n",
    "    for stroke in raw_strokes:\n",
    "        for i in range(len(stroke[0]) - 1):\n",
    "            _ = cv2.line(img, (stroke[0][i], stroke[1][i]), (stroke[0][i + 1], stroke[1][i + 1]), 255, lw)\n",
    "    if size != BASE_SIZE:\n",
    "        return cv2.resize(img, (size, size))\n",
    "    else:\n",
    "        return img\n",
    "\n",
    "#ADD DATA AUGMENTATION TO BOOST\n",
    "def image_generator(size, batchsize, ks, lw=6):\n",
    "    while True:\n",
    "        for k in np.random.permutation(ks):\n",
    "            filename = os.path.join(DP_DIR, 'train_k{}.csv.gz'.format(k))\n",
    "            for df in pd.read_csv(filename, chunksize=batchsize):\n",
    "                df['drawing'] = df['drawing'].apply(ast.literal_eval)\n",
    "                x = np.zeros((len(df), size, size))\n",
    "                for i, raw_strokes in enumerate(df.drawing.values):\n",
    "                    x[i] = draw_cv2(raw_strokes, size=size, lw=lw)\n",
    "                x = x / 255.\n",
    "                x = x.reshape((len(df), size, size, 1)).astype(np.float32)\n",
    "                y = tf.keras.utils.to_categorical(df.y, num_classes=NCATS)\n",
    "                yield x, y\n",
    "\n",
    "def df_to_image_array(df, size=size, lw=6):\n",
    "    df['drawing'] = df['drawing'].apply(ast.literal_eval)\n",
    "    x = np.zeros((len(df), size, size))\n",
    "    for i, raw_strokes in enumerate(df.drawing.values):\n",
    "        x[i] = draw_cv2(raw_strokes, size=size, lw=lw)\n",
    "    x = x / 255.\n",
    "    x = x.reshape((len(df), size, size, 1)).astype(np.float32)\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a6b041e1365a69cc0576e081e1e7db9c5928742e"
   },
   "outputs": [],
   "source": [
    "STEPS = 1000\n",
    "batchsize = 512\n",
    "epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "18162bacd65b3b8d25107d58a738a92edd415f30"
   },
   "outputs": [],
   "source": [
    "valid_df = pd.read_csv(os.path.join(DP_DIR, 'train_k{}.csv.gz'.format(NCSVS - 1)), nrows=30000)\n",
    "x_valid = df_to_image_array(valid_df, size)\n",
    "y_valid = tf.keras.utils.to_categorical(valid_df.y, num_classes=NCATS)\n",
    "print(x_valid.shape, y_valid.shape)\n",
    "print('Validation array memory {:.2f} GB'.format(x_valid.nbytes / 1024.**3 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "10ddcecba462b047aa8dc74008615cde7452ad63"
   },
   "outputs": [],
   "source": [
    "train_datagen = image_generator(size=size, batchsize=batchsize, ks=range(NCSVS - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5b11507f5b2d5e3f33462c59db06fd79282ef96e"
   },
   "outputs": [],
   "source": [
    "base_model = MobileNet(input_shape=(size, size, 1), include_top=False, weights=None, classes=n_labels)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(n_labels, activation='softmax')(x)\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cca5f88517c8d7c935c68b36082d4959828718b8"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy',\n",
    "              metrics=[categorical_crossentropy, categorical_accuracy, top_3_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f997c27a6f3ac201e35f0947bc4db60d9dc44124"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b702dbec4f03127909f3c0fab9d48b86c265c74f"
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.5, patience=5,\n",
    "                      min_delta=0.005, mode='max', cooldown=3, verbose=1)\n",
    "]\n",
    "\n",
    "hist = model.fit_generator(\n",
    "    train_datagen, steps_per_epoch=STEPS, epochs=epochs, verbose=1,\n",
    "    validation_data=(x_valid, y_valid),\n",
    "    callbacks = callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "97e830678323a599deb8fb4b22fd1d986e893aaa"
   },
   "outputs": [],
   "source": [
    "def gen_graph(history, title):\n",
    "    plt.plot(history.history['categorical_accuracy'])\n",
    "    plt.plot(history.history['val_categorical_accuracy'])\n",
    "    plt.plot(history.history['top_3_accuracy'])\n",
    "    plt.plot(history.history['val_top_3_accuracy'])\n",
    "    plt.title('Accuracy ' + title)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train', 'validation', 'Test top 3', 'Validation top 3'], loc='upper left')\n",
    "    plt.show()\n",
    "    plt.plot(history.history['categorical_crossentropy'])\n",
    "    plt.plot(history.history['val_categorical_crossentropy'])\n",
    "    plt.title('Loss ' + title)\n",
    "    plt.ylabel('MLogLoss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a6404ecead97faa36c8cf7f6cfacefb5b5f3d920"
   },
   "outputs": [],
   "source": [
    "#plot\n",
    "gen_graph(hist, \n",
    "              \"Simple net lul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d36a3540618ccba3308a8d33fb948db012b88dc5"
   },
   "outputs": [],
   "source": [
    "pred_results = []\n",
    "chunksize = 10000\n",
    "reader = pd.read_csv(INPUT_DIR + 'test_simplified.csv', chunksize=chunksize)\n",
    "for chunk in tqdm(reader):\n",
    "    imgs = df_to_image_array(chunk)\n",
    "    pred = model.predict(imgs, verbose=1)\n",
    "    top_3 =  np.argsort(-pred)[:, 0:3]  \n",
    "    pred_results.append(top_3)\n",
    "print(\"Finished test predictions...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "85acaa27c8c28458f7c132279ea4a3e11596b178"
   },
   "outputs": [],
   "source": [
    "#prepare data for saving\n",
    "reverse_dict = {v: k for k, v in class_dict.items()}\n",
    "pred_results = np.concatenate(pred_results)\n",
    "print(\"Finished data prep...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e962bffec419cf26be76577c4185fef0532b0cd6"
   },
   "outputs": [],
   "source": [
    "preds_df = pd.DataFrame({'first': pred_results[:,0], 'second': pred_results[:,1], 'third': pred_results[:,2]})\n",
    "preds_df = preds_df.replace(reverse_dict)\n",
    "\n",
    "preds_df['words'] = preds_df['first'] + \" \" + preds_df['second'] + \" \" + preds_df['third']\n",
    "\n",
    "sub = pd.read_csv(INPUT_DIR + 'sample_submission.csv', index_col=['key_id'])\n",
    "sub['word'] = preds_df.words.values\n",
    "sub.to_csv('1class_per_label_proto.csv')\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "97f621ecfec4e794b4b4c44c6e72415a33de4a1a"
   },
   "outputs": [],
   "source": [
    "endTime = time.time()\n",
    "print(endTime - startTime)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
